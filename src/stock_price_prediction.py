# -*- coding: utf-8 -*-
"""3014318_Stock_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xh9rW1QTlmIzGbTOuDabEf105aSmZgiq

### Introduction

In this notebook, I tackle a multivariate time series forecasting task using deep learning. The goal is to train a model that can accurately predict stock prices for 442 companies on April 1st, 2022, using historical price data.

The process involves the following key steps:

- Preparing the time series data for modeling  
- Building and training an LSTM-based recurrent neural network  
- Using Optuna for hyperparameter optimization to minimize prediction error (MSE)  
- Generating final predictions and saving them in the required submission format  
- Applying Captum to interpret the model and understand which features (companies or time steps) had the most influence on the forecast

I use the RNN Lab 7 as a foundational starting point and adapt it to support multivariate inputs, hyperparameter tuning, and model interpretability.

### Data Loading and Preprocessing

In this section, I load the dataset and prepare it for training a time series model.

The original dataset contains daily stock prices, where each **row represents a company**, and each **column corresponds to a date**. Since I need the data in a time-series format where **each row is a day**, I transpose the DataFrame. This makes each row a time step, and each column a feature (company), which is the correct structure for feeding into an LSTM.

To ensure the model trains efficiently and handles different company scales fairly, I normalize each company's price series individually using `MinMaxScaler`, scaling their values between 0 and 1. I also store each scaler in a dictionary so I can reverse the normalization later when generating predictions.

After normalization, I construct training sequences using a sliding window approach:
- Each input sequence is made of 60 consecutive days of prices (`WINDOW_SIZE = 60`)
- The target is the price of all companies on the **next day** (the 61st day)

This results in multivariate input-output pairs, where each sample captures 60 days of prices for all 442 companies, and the corresponding label is the next day’s price for those same companies.

Once the sequences are created, I convert them into PyTorch tensors and split the data into training and validation sets (80% training, 20% validation).

Finally, I define a helper function `create_dataloader` that wraps the data in a `DataLoader` to make batching easy, especially during Optuna-based hyperparameter tuning.
"""

# Install Optuna (only needed once in the environment)
!pip install optuna

# Import necessary libraries
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import optuna
from sklearn.preprocessing import MinMaxScaler
from torch.utils.data import TensorDataset, DataLoader

# Step 1: Load and reshape the dataset
# The original dataset has companies as rows and dates as columns
# We transpose it so that each row is a time step, and each column is a company
raw_data = pd.read_csv("train.csv")
data_by_date = raw_data.set_index("ID").T
data_by_date.index = pd.to_datetime(data_by_date.index, dayfirst=True)

# Step 2: Normalize each company’s price series individually using MinMaxScaler
# This ensures that all input features (companies) are scaled between 0 and 1
normalized_data = data_by_date.copy()
scaler_dict = {}

for company in data_by_date.columns:
    scaler = MinMaxScaler()
    normalized_data[company] = scaler.fit_transform(data_by_date[[company]])
    scaler_dict[company] = scaler

# Step 3: Create sequences for time series forecasting
# Each input is a window of 60 time steps (days), target is the next day's price for all companies
WINDOW_SIZE = 60
input_sequences = []
target_sequences = []

for i in range(len(normalized_data) - WINDOW_SIZE):
    input_window = normalized_data.iloc[i:i+WINDOW_SIZE].values  # Shape: [60, num_companies]
    target_day = normalized_data.iloc[i+WINDOW_SIZE].values       # Shape: [num_companies]
    input_sequences.append(input_window)
    target_sequences.append(target_day)

# Convert sequences to PyTorch tensors
X_all = torch.tensor(np.array(input_sequences), dtype=torch.float32)
Y_all = torch.tensor(np.array(target_sequences), dtype=torch.float32)

# Step 4: Split into training and validation datasets (80% train, 20% validation)
split_index = int(len(X_all) * 0.8)
X_train, X_val = X_all[:split_index], X_all[split_index:]
Y_train, Y_val = Y_all[:split_index], Y_all[split_index:]

# Step 5: Create a helper function to prepare PyTorch DataLoaders
# This allows us to easily change batch size during Optuna optimization
def create_dataloader(X_tensor, Y_tensor, batch_size):
    dataset = TensorDataset(X_tensor, Y_tensor)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True)

"""### Model Definition and Hyperparameter Optimization

To model the sequential nature of stock price data, I use an LSTM (Long Short-Term Memory) network. LSTMs are well-suited for time series forecasting tasks because they are capable of capturing long-term dependencies across sequential inputs.

I define a class `StockLSTM` that takes in:
- `input_dim`: the number of features (i.e., companies)
- `hidden_dim`: the size of the LSTM's hidden state
- `output_dim`: the number of target variables (again, companies)
- `num_layers`: the number of stacked LSTM layers
- `dropout`: the dropout probability between layers to reduce overfitting

The model processes each input sequence and returns predictions for the next time step, using only the output from the final time step in the sequence.

To improve model performance, I use **Optuna**, an automated hyperparameter optimization library. I define an `objective()` function that:
- Suggests values for key hyperparameters (hidden size, number of layers, dropout, learning rate, batch size)
- Trains the model using the suggested configuration
- Evaluates it on the validation set
- Returns the validation loss (MSE), which Optuna tries to minimize

I run the study for 20 trials. Each trial represents a different combination of hyperparameters. After optimization, I print the best combination found by Optuna for use in final model training.

"""

# Step 6: Define the LSTM forecasting model
# The model takes in a sequence of stock prices and predicts the next time step
class StockLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):
        super(StockLSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,
                            batch_first=True, dropout=dropout)
        self.output_layer = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        # Initialize hidden state and cell state with zeros (no prior memory)
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)

        # Forward pass through LSTM
        out, _ = self.lstm(x, (h0, c0))  # out: [batch_size, time_steps, hidden_dim]
        last_timestep = out[:, -1, :]    # Only use output from final time step
        output = self.output_layer(last_timestep)
        return output

# Step 7: Define the Optuna objective function to search for the best hyperparameters
# This function builds a model with trial parameters, trains it, and returns the validation loss
def objective(trial):
    # Suggest a set of hyperparameters
    hidden_dim = trial.suggest_int("hidden_dim", 32, 256)
    num_layers = trial.suggest_int("num_layers", 1, 5)
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])

    # Initialize the model using trial parameters
    model = StockLSTM(
        input_dim=X_all.shape[2],
        hidden_dim=hidden_dim,
        output_dim=Y_all.shape[1],
        num_layers=num_layers,
        dropout=dropout
    )

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Prepare DataLoaders using the current batch size
    train_loader = create_dataloader(X_train, Y_train, batch_size)
    val_loader = create_dataloader(X_val, Y_val, batch_size)

    # Train model
    for epoch in range(10):
        model.train()
        for x_batch, y_batch in train_loader:
            optimizer.zero_grad()
            predictions = model(x_batch)
            loss = criterion(predictions, y_batch)
            loss.backward()
            optimizer.step()

    # Evaluate on validation data and return the average loss
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for x_batch, y_batch in val_loader:
            predictions = model(x_batch)
            total_loss += criterion(predictions, y_batch).item()

    average_val_loss = total_loss / len(val_loader)
    return average_val_loss

# Step 8: Launch Optuna study to find the best combination of hyperparameters
study = optuna.create_study(direction="minimize")  # We want to minimize validation loss
study.optimize(objective, n_trials=20)  # We can increase n_trials for better search

# Step 9: Output the best hyperparameters found
print("Best Hyperparameters Found:")
print(study.best_params)

"""### Final Model Training

Once Optuna has identified the best hyperparameters, I extract them and use them to initialize the final model configuration. This includes the optimal hidden size, number of LSTM layers, dropout rate, learning rate, and batch size.

Using these optimal values, I construct the final `StockLSTM` model and train it on the full training dataset. This time, I train the model for a larger number of epochs (50) since this is no longer part of the tuning process — it's the real training phase intended to produce the final predictions.

During training, I use the Mean Squared Error (MSE) loss function, which aligns with the competition's evaluation metric. I also print the average training loss every 5 epochs to monitor convergence and training stability.

By the end of this phase, the model should be well-optimized and ready to generate predictions for April 1st, 2022.

"""

# Step 10: Retrieve best hyperparameters from the completed Optuna study
optimal_params = study.best_params
print("Best Hyperparameters Selected by Optuna:")
print(optimal_params)

# Step 11: Initialize the final model using the best parameters
final_model = StockLSTM(
    input_dim=X_all.shape[2],  # number of features (companies)
    hidden_dim=optimal_params["hidden_dim"],
    output_dim=Y_all.shape[1],  # same as input_dim in this case
    num_layers=optimal_params["num_layers"],
    dropout=optimal_params["dropout"]
)

# Define loss function and optimizer for training
loss_function = nn.MSELoss()
optimizer = optim.Adam(final_model.parameters(), lr=optimal_params["learning_rate"])

# Create training DataLoader using best batch size
final_train_loader = create_dataloader(X_train, Y_train, optimal_params["batch_size"])

# Step 12: Train the final model on the full training dataset
# More epochs here since this is not tuning anymore — it's the real training
for epoch in range(50):
    final_model.train()
    epoch_loss = 0

    for features, targets in final_train_loader:
        optimizer.zero_grad()
        predictions = final_model(features)
        loss = loss_function(predictions, targets)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    # Print loss every 5 epochs for monitoring
    if epoch % 5 == 0:
        avg_epoch_loss = epoch_loss / len(final_train_loader)
        print(f"Epoch {epoch}/50 - Avg Training Loss: {avg_epoch_loss:.6f}")

"""### Final Prediction and Inverse Transformation

After training the final model, I save its weights to disk using `torch.save`. This allows me to reuse or deploy the model later without retraining.

To make a prediction for April 1st, 2022, I use the last 60 days of normalized input data and pass it into the trained model. Since the model expects a batch of sequences, I reshape the input to match the required shape `[1, 60, num_companies]`.

The model returns the predicted stock prices for the next day, but in the normalized scale. To interpret these predictions in their original financial context, I apply the inverse transformation using the `MinMaxScaler` instances that were saved earlier for each company. This step restores the predicted prices to their original scale.

Finally, I wrap the results into a DataFrame for easy display and potential saving. The output shows the predicted prices for all 442 companies for the next trading day following the last date in the dataset.

"""

# Step 13: Save the trained model to disk for future reuse or deployment
torch.save(final_model.state_dict(), "final_lstm_model.pth")

# Step 14: Predict the next day's prices using the last 60 days of known data
# We use the final trained model and pass in the most recent input sequence
final_model.eval()  # Set model to evaluation mode (no dropout, no gradients)

with torch.no_grad():
    # Extract the last 60 days of scaled input data
    latest_window = torch.tensor(normalized_data.iloc[-WINDOW_SIZE:].values, dtype=torch.float32)
    latest_window = latest_window.unsqueeze(0)  # Shape: [1, 60, num_companies]

    # Predict next day's prices in scaled form
    predicted_scaled_output = final_model(latest_window)  # Shape: [1, num_companies]

# Step 15: Reverse the MinMax scaling to get predicted prices in original scale
# We'll use the individual scalers stored earlier for each company
predicted_original = np.zeros_like(predicted_scaled_output.cpu().numpy())

for idx, company in enumerate(normalized_data.columns):
    company_scaled_value = predicted_scaled_output.cpu().numpy()[:, idx].reshape(-1, 1)
    predicted_original[0, idx] = scaler_dict[company].inverse_transform(company_scaled_value)[0, 0]

# Step 16: Wrap the results into a DataFrame for easy readability or saving
predicted_prices_df = pd.DataFrame(predicted_original, columns=normalized_data.columns)

print("Predicted Stock Prices for the Next Day:")
print(predicted_prices_df)

"""### Submission Preparation

To meet the required submission format, I reshape the predicted prices into the correct structure. The expected format has:

- One row per company (using the company ID as the index)
- A single column named `"value"` that holds the predicted price

To achieve this, I transpose the prediction DataFrame so that each company becomes a row, and then rename the column appropriately. I also set the index name to `"ID"` to match the submission template.

Finally, I export the DataFrame to a CSV file named `predicted_prices.csv`, which is ready for submission.

"""

# Step 17: Prepare submission DataFrame in the required format
# Transpose to have company IDs as index and 'value' as the column name
submission_df = predicted_prices_df.T
submission_df.columns = ["value"]
submission_df.index.name = "ID"

# Step 18: Save the submission to a CSV file
submission_df.to_csv("predicted_prices.csv")
print("Submission file saved as: predicted_prices.csv")

"""### Model Interpretation with Captum (Integrated Gradients)

I use Captum to interpret the predictions made by the trained LSTM model. The goal is to understand which features (companies) and which time steps (days in the 60-day input window) had the greatest influence on the model's output.

This is done through **attribution**, which refers to assigning importance scores to input features based on how much they contributed to the model’s prediction. In this case, each attribution value represents how strongly a particular company's stock price on a particular day influenced the predicted prices for April 1st, 2022.

To compute attributions, I apply **Integrated Gradients** (IG), a method that averages gradients along a path from a baseline input (e.g., all zeros) to the actual input. This approach provides a more stable and theoretically grounded measure of feature importance compared to raw gradients.

I define a forward function that outputs a single scalar by summing the model's predictions across all companies. This is required for attribution calculations.

Two visualizations are produced:

**1. Feature Importance (Company-Level)**  
A bar plot showing the total attribution for each company. This reveals which companies had the most influence on the prediction.

**2. Time Step Importance (Temporal-Level)**  
A line plot showing how important each day in the 60-day sequence was. This helps assess whether the model relies more on recent data, which is expected behavior for time series forecasting.

These interpretability results help validate whether the model is learning reasonable patterns or relying on spurious signals.

"""

# ----------------------------
# CAPTUM INTERPRETATION
# ----------------------------

#!pip install captum

# Step 19: Interpret the model using Captum (Integrated Gradients)
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt

# Use the same latest input window for interpretation
latest_window.requires_grad = True  # Enable gradient tracking

# Define a forward function that returns a single output tensor for Captum
# This version sums the predictions across all companies to produce a scalar attribution target
# (Alternatively, we can select a specific company output if desired)
def forward_func(input_tensor):
    return final_model(input_tensor).sum().unsqueeze(0)

# Initialize Captum Integrated Gradients
ig = IntegratedGradients(forward_func)

# Compute attributions
attributions, delta = ig.attribute(latest_window, return_convergence_delta=True)

# Convert to numpy for visualization
attributions_np = attributions.squeeze(0).detach().cpu().numpy()  # Shape: [60, num_companies]

# ------ Option 1: Visualize feature (company) importance ------
feature_importance = np.abs(attributions_np).sum(axis=0)  # Sum over time steps
plt.figure(figsize=(12, 4))
plt.bar(range(len(feature_importance)), feature_importance)
plt.title("Feature Importance (Companies)")
plt.xlabel("Company Index")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()

# ------ Option 2: Visualize time step importance ------
time_step_importance = np.abs(attributions_np).sum(axis=1)  # Sum over companies
plt.figure(figsize=(12, 4))
plt.plot(time_step_importance)
plt.title("Time Step Importance (Last 60 Days)")
plt.xlabel("Time Step (Day)")
plt.ylabel("Importance")
plt.tight_layout()
plt.show()

"""#### 1. Feature Importance (Companies)

This bar plot shows the total attribution for each of the 442 companies, aggregated across the entire 60-day input window. Each bar represents how much a single company contributed to the model's final prediction.

**Interpretation:**
- The model does not treat all companies equally. A number of companies have significantly higher attribution values.
- Certain companies (for example, around indices 60, 150, and above 430) stand out as especially influential.
- Companies with very low attribution likely had little to no impact on the predicted prices.

This is consistent with real-world financial behavior, where some companies act as stronger indicators of market trends or are more volatile, making them more influential in forecasting.

---

#### 2. Time Step Importance (Last 60 Days)

This line plot shows the aggregated attribution for each time step (day) across all companies. It helps identify which days within the 60-day input sequence were most influential.

**Interpretation:**
- The attribution values increase significantly as we move closer to the most recent days.
- The model relies most heavily on the last 10 days (especially days 50 to 59), which aligns with expectations in short-term stock forecasting.
- Earlier time steps have minimal influence, indicating the model is effectively learning to prioritize recent market behavior.

---

These attribution results confirm that the model is behaving in a reasonable and interpretable way, focusing more on recent price movements and placing emphasis on a select group of companies.

### Time-Wise Attribution for a Single Company

To explore how the model's attention changes over time for a specific company, I compute time-wise attributions using IG. For demonstration purposes, I selected Company 100 to explore how attribution evolves over time for an individual stock. This is an arbitrary example and not necessarily among the top contributors.


The forward function is modified to isolate the prediction corresponding to that single company. The resulting attribution values indicate how much each of the 60 input time steps contributed to the predicted price of that specific company on April 1st, 2022.

**Key observations:**
- Attribution values are extremely small, but the **relative trend** is informative.
- The plot reveals a steady increase in attribution starting around day 30, with a clear peak between days 48–54.
- The most recent 10–15 days are significantly more influential than earlier time steps.

This behavior suggests that for Company 100, the model places strong emphasis on recent price activity when forecasting future prices. This aligns with standard expectations in time series forecasting, especially in financial domains where recent movements tend to carry more predictive weight.
"""

# Let's pick company index 100 (replace with any index)
company_idx = 100

def forward_func_company(input_tensor):
    return final_model(input_tensor)[0, company_idx].unsqueeze(0)

ig_company = IntegratedGradients(forward_func_company)
attr_company, _ = ig_company.attribute(latest_window, return_convergence_delta=True)
attr_company_np = attr_company.squeeze(0).detach().cpu().numpy()

plt.figure(figsize=(12, 4))
plt.plot(range(WINDOW_SIZE), attr_company_np[:, company_idx])
plt.title(f"Time-wise Attribution for Company {company_idx}")
plt.xlabel("Time Step")
plt.ylabel("Attribution")
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Attribution Comparison: IG vs Saliency

To deepen the interpretability of the model's predictions, I compute and compare attributions using two methods from the Captum library:

- **IG:** A path-based method that averages gradients between a baseline input and the actual input.
- **Saliency:** A simpler, sensitivity-based method that computes the raw gradient of the output with respect to each input feature.

---

#### 1. Heatmaps (Time × Company)

I generate heatmaps to visualize the attribution values across both time and company dimensions.

- The **IG heatmap** highlights regions where the model assigns high importance based on accumulated gradients.
- The **Saliency heatmap** shows more granular sensitivity, though it's often noisier.

These heatmaps help identify which companies and which days in the input window most strongly influenced the prediction.

---

#### 2. Top 10 Most Influential Companies

I calculate the total attribution for each company by summing over all time steps, separately for IG and Saliency.

- The **bar plots** display the top 10 companies that contributed the most.
- While both methods highlight influential companies, IG tends to produce more stable, interpretable results, while Saliency is more reactive to sharp gradients.

---

#### 3. Time Step Importance

By summing attributions across companies for each time step, I evaluate how much influence each day in the 60-day window had.

- Both methods show that the **most recent days** were more important for prediction.
- IG again shows smoother attribution, while Saliency spikes where gradients are steeper.

---

#### 4. IG vs. Saliency Attribution Comparison

Finally, I overlay the total attribution values per company from both methods to visualize the overall differences.

- **IGs** provide a more structured and stable attribution profile.
- **Saliency** values are smaller and noisier but still helpful for identifying sensitivity.

---

This comparative analysis confirms that while both methods reveal meaningful insights, Integrated Gradients offer more reliable attributions for deep sequence models like LSTMs. Saliency can still be useful to highlight local sensitivities.

"""

from captum.attr import IntegratedGradients, Saliency
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Set model to evaluation mode
final_model.eval()

# Ensure input tensor requires gradients
latest_window.requires_grad = True

# ----------------------------
# Forward function for Captum
# ----------------------------
# We use the sum of all predicted company outputs as the target
# This creates a single scalar output, required for attribution
def forward_func(input_tensor):
    return final_model(input_tensor).sum().unsqueeze(0)

# ----------------------------
# Integrated Gradients
# ----------------------------
ig = IntegratedGradients(forward_func)
attributions_ig, _ = ig.attribute(latest_window, return_convergence_delta=True)
attributions_ig_np = attributions_ig.squeeze(0).detach().cpu().numpy()  # [60, num_companies]

# ----------------------------
# Saliency
# ----------------------------
saliency = Saliency(forward_func)
attributions_sal = saliency.attribute(latest_window)
attributions_sal_np = attributions_sal.squeeze(0).detach().cpu().numpy()  # [60, num_companies]

# ----------------------------
# Heatmaps: Time x Company
# ----------------------------

# Integrated Gradients Heatmap
plt.figure(figsize=(16, 6))
sns.heatmap(attributions_ig_np, cmap='coolwarm', center=0)
plt.title("Integrated Gradients Heatmap (Time Steps × Companies)")
plt.xlabel("Company Index")
plt.ylabel("Time Step (0 = Oldest, 59 = Most Recent)")
plt.tight_layout()
plt.show()

# Saliency Heatmap
plt.figure(figsize=(16, 6))
sns.heatmap(attributions_sal_np, cmap='coolwarm', center=0)
plt.title("Saliency Heatmap (Time Steps × Companies)")
plt.xlabel("Company Index")
plt.ylabel("Time Step (0 = Oldest, 59 = Most Recent)")
plt.tight_layout()
plt.show()

# ----------------------------
# Feature (Company) Importance
# ----------------------------

# Sum absolute attribution over time to get company-level importance
feature_importance_ig = np.abs(attributions_ig_np).sum(axis=0)
feature_importance_sal = np.abs(attributions_sal_np).sum(axis=0)

# Top 10 most important companies
top10_idx_ig = np.argsort(feature_importance_ig)[-10:][::-1]
top10_idx_sal = np.argsort(feature_importance_sal)[-10:][::-1]

# IG
plt.figure(figsize=(10, 4))
plt.bar(range(10), feature_importance_ig[top10_idx_ig])
plt.title("Top 10 Influential Companies (Integrated Gradients)")
plt.xlabel("Company Index")
plt.ylabel("Total Attribution")
plt.xticks(range(10), top10_idx_ig)
plt.tight_layout()
plt.show()

# Saliency
plt.figure(figsize=(10, 4))
plt.bar(range(10), feature_importance_sal[top10_idx_sal])
plt.title("Top 10 Influential Companies (Saliency)")
plt.xlabel("Company Index")
plt.ylabel("Gradient Magnitude")
plt.xticks(range(10), top10_idx_sal)
plt.tight_layout()
plt.show()

# ----------------------------
# Time-Step Importance
# ----------------------------

# Sum absolute attribution over features to get time-step importance
time_importance_ig = np.abs(attributions_ig_np).sum(axis=1)
time_importance_sal = np.abs(attributions_sal_np).sum(axis=1)

# IG
plt.figure(figsize=(12, 4))
plt.plot(time_importance_ig)
plt.title("Time Step Importance (Integrated Gradients)")
plt.xlabel("Time Step")
plt.ylabel("Total Attribution")
plt.grid(True)
plt.tight_layout()
plt.show()

# Saliency
plt.figure(figsize=(12, 4))
plt.plot(time_importance_sal)
plt.title("Time Step Importance (Saliency)")
plt.xlabel("Time Step")
plt.ylabel("Gradient Magnitude")
plt.grid(True)
plt.tight_layout()
plt.show()

# ----------------------------
# IG vs. Saliency Comparison
# ----------------------------

plt.figure(figsize=(12, 4))
plt.plot(feature_importance_ig, label="Integrated Gradients")
plt.plot(feature_importance_sal, label="Saliency", alpha=0.7)
plt.title("Company-Level Attribution Comparison: IG vs Saliency")
plt.xlabel("Company Index")
plt.ylabel("Attribution Magnitude")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Heatmap Comparison: IGs vs Saliency

The above heatmaps display attribution values across 60 input time steps (rows) and 442 companies (columns), using two different interpretability methods:

---

#### IGs (Top Heatmap)
- Shows smoother, structured attributions.
- Most of the influence is concentrated in the most recent time steps (bottom rows), which aligns with expected behavior in short-term forecasting.
- A few companies consistently show high positive or negative attribution (highlighted in deep red/blue), indicating stronger contributions to the model’s output.

---

#### Saliency (Bottom Heatmap)
- Much lower magnitude values overall (as seen in the color bar scale).
- Attribution values are more scattered and appear noisier.
- Still captures the pattern that recent time steps matter more, but with less clarity and more sensitivity to small changes in input.

---

While both methods suggest that the most recent days and a subset of companies are influential, **IG** provides clearer and more stable attribution. **Saliency** is useful for quick sensitivity analysis but lacks the consistency and robustness of IG for deeper interpretability in sequential models like LSTMs.

### Top 10 Influential Companies: IG vs. Saliency

These two bar charts compare the most influential companies identified by two different attribution methods:

---

#### IGs (Top Plot)
- The companies shown here (e.g., indices 420, 142, 53) have the highest total attributions based on Integrated Gradients.
- This result reflects IG's strength in capturing cumulative influence across input paths, offering more stable and interpretable attributions.

---

#### Saliency (Bottom Plot)
- The top companies from Saliency (e.g., indices 11, 105, 298) differ completely from the IG results.
- Saliency values are much lower and more uniformly distributed, indicating less certainty in which companies are most important.
- This is expected, as Saliency captures raw gradient sensitivity, which can fluctuate and be affected by small input perturbations.

---

### Time Step Importance: Integrated Gradients vs Saliency

These two plots show how much each time step (day in the 60-day input window) contributed to the model's prediction. Attribution is summed across all companies for each day, using two different methods:

---

#### Integrated Gradients (Top Plot)
- Shows a clear increase in attribution as time progresses.
- Peaks between time steps **50–55**, indicating that the model places the highest importance on the most recent 10 days.
- The curve is smooth and interpretable, reflecting consistent behavior expected from a model trained on time series data.

---

#### Saliency (Bottom Plot)
- Also shows increased importance in recent time steps, particularly after time step **50**.
- However, the scale of attribution is much lower overall and the curve is steeper toward the end, with very little contribution from earlier days.
- As a gradient-based sensitivity method, Saliency tends to produce sharper, less distributed signals.

---
Both methods confirm that the model focuses on recent days when making predictions. However, **Integrated Gradients** offers a more balanced and interpretable attribution profile, while **Saliency** highlights more localized sensitivity.

### Company-Level Attribution Comparison: Integrated Gradients vs Saliency

This line plot compares the total attribution magnitude assigned to each of the 442 companies using two different interpretability methods:

- **IG:** Shown in blue
- **Saliency:** Shown in orange

Each point along the x-axis corresponds to a company index, and the y-axis represents how influential that company was in the model’s prediction for April 1st, 2022.

**Key observations:**
- **Integrated Gradients** reveals a wide and meaningful spread of attribution values across companies, with several sharp peaks indicating strongly influential companies.
- **Saliency** values are significantly lower and relatively flat, suggesting that the model appears less sensitive at a gradient level, or that raw gradients fail to capture deeper feature interactions.
- The results confirm that IG provides more expressive and stable attributions, especially for understanding long-range dependencies in sequence models like LSTMs.
"""